{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machines on MovieLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download ml-100k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ml-100k.zip\n",
      "   creating: ml-100k/\n",
      "  inflating: ml-100k/allbut.pl       \n",
      "  inflating: ml-100k/mku.sh          \n",
      "  inflating: ml-100k/README          \n",
      "  inflating: ml-100k/u.data          \n",
      "  inflating: ml-100k/u.genre         \n",
      "  inflating: ml-100k/u.info          \n",
      "  inflating: ml-100k/u.item          \n",
      "  inflating: ml-100k/u.occupation    \n",
      "  inflating: ml-100k/u.user          \n",
      "  inflating: ml-100k/u1.base         \n",
      "  inflating: ml-100k/u1.test         \n",
      "  inflating: ml-100k/u2.base         \n",
      "  inflating: ml-100k/u2.test         \n",
      "  inflating: ml-100k/u3.base         \n",
      "  inflating: ml-100k/u3.test         \n",
      "  inflating: ml-100k/u4.base         \n",
      "  inflating: ml-100k/u4.test         \n",
      "  inflating: ml-100k/u5.base         \n",
      "  inflating: ml-100k/u5.test         \n",
      "  inflating: ml-100k/ua.base         \n",
      "  inflating: ml-100k/ua.test         \n",
      "  inflating: ml-100k/ub.base         \n",
      "  inflating: ml-100k/ub.test         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-05-21 13:31:10--  http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
      "Resolving files.grouplens.org (files.grouplens.org)... 128.101.65.152\n",
      "Connecting to files.grouplens.org (files.grouplens.org)|128.101.65.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4924029 (4.7M) [application/zip]\n",
      "Saving to: ‘ml-100k.zip’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  1%  763K 6s\n",
      "    50K .......... .......... .......... .......... ..........  2% 1.49M 5s\n",
      "   100K .......... .......... .......... .......... ..........  3% 61.9M 3s\n",
      "   150K .......... .......... .......... .......... ..........  4% 64.3M 2s\n",
      "   200K .......... .......... .......... .......... ..........  5% 1.52M 2s\n",
      "   250K .......... .......... .......... .......... ..........  6%  103M 2s\n",
      "   300K .......... .......... .......... .......... ..........  7%  114M 2s\n",
      "   350K .......... .......... .......... .......... ..........  8% 67.3M 1s\n",
      "   400K .......... .......... .......... .......... ..........  9% 1.35M 2s\n",
      "   450K .......... .......... .......... .......... .......... 10%  283M 1s\n",
      "   500K .......... .......... .......... .......... .......... 11%  299M 1s\n",
      "   550K .......... .......... .......... .......... .......... 12%  262M 1s\n",
      "   600K .......... .......... .......... .......... .......... 13%  335M 1s\n",
      "   650K .......... .......... .......... .......... .......... 14%  280M 1s\n",
      "   700K .......... .......... .......... .......... .......... 15%  294M 1s\n",
      "   750K .......... .......... .......... .......... .......... 16%  275M 1s\n",
      "   800K .......... .......... .......... .......... .......... 17% 1.50M 1s\n",
      "   850K .......... .......... .......... .......... .......... 18%  278M 1s\n",
      "   900K .......... .......... .......... .......... .......... 19% 49.7M 1s\n",
      "   950K .......... .......... .......... .......... .......... 20%  309M 1s\n",
      "  1000K .......... .......... .......... .......... .......... 21% 54.5M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 22%  263M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 23%  321M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 24%  310M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 25%  309M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 27% 1.18M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 28%  134M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 29%  299M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 30%  147M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 31%  187M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 32%  187M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 33%  306M 0s\n",
      "  1600K .......... .......... .......... .......... .......... 34%  301M 0s\n",
      "  1650K .......... .......... .......... .......... .......... 35%  325M 0s\n",
      "  1700K .......... .......... .......... .......... .......... 36%  262M 0s\n",
      "  1750K .......... .......... .......... .......... .......... 37%  306M 0s\n",
      "  1800K .......... .......... .......... .......... .......... 38%  290M 0s\n",
      "  1850K .......... .......... .......... .......... .......... 39%  337M 0s\n",
      "  1900K .......... .......... .......... .......... .......... 40%  254M 0s\n",
      "  1950K .......... .......... .......... .......... .......... 41%  298M 0s\n",
      "  2000K .......... .......... .......... .......... .......... 42%  316M 0s\n",
      "  2050K .......... .......... .......... .......... .......... 43%  301M 0s\n",
      "  2100K .......... .......... .......... .......... .......... 44%  264M 0s\n",
      "  2150K .......... .......... .......... .......... .......... 45%  291M 0s\n",
      "  2200K .......... .......... .......... .......... .......... 46%  305M 0s\n",
      "  2250K .......... .......... .......... .......... .......... 47%  318M 0s\n",
      "  2300K .......... .......... .......... .......... .......... 48% 1.49M 0s\n",
      "  2350K .......... .......... .......... .......... .......... 49%  267M 0s\n",
      "  2400K .......... .......... .......... .......... .......... 50%  273M 0s\n",
      "  2450K .......... .......... .......... .......... .......... 51%  314M 0s\n",
      "  2500K .......... .......... .......... .......... .......... 53%  266M 0s\n",
      "  2550K .......... .......... .......... .......... .......... 54%  315M 0s\n",
      "  2600K .......... .......... .......... .......... .......... 55%  340M 0s\n",
      "  2650K .......... .......... .......... .......... .......... 56%  300M 0s\n",
      "  2700K .......... .......... .......... .......... .......... 57%  267M 0s\n",
      "  2750K .......... .......... .......... .......... .......... 58% 75.7M 0s\n",
      "  2800K .......... .......... .......... .......... .......... 59%  290M 0s\n",
      "  2850K .......... .......... .......... .......... .......... 60%  287M 0s\n",
      "  2900K .......... .......... .......... .......... .......... 61%  171M 0s\n",
      "  2950K .......... .......... .......... .......... .......... 62%  275M 0s\n",
      "  3000K .......... .......... .......... .......... .......... 63%  263M 0s\n",
      "  3050K .......... .......... .......... .......... .......... 64%  287M 0s\n",
      "  3100K .......... .......... .......... .......... .......... 65%  200M 0s\n",
      "  3150K .......... .......... .......... .......... .......... 66%  334M 0s\n",
      "  3200K .......... .......... .......... .......... .......... 67%  264M 0s\n",
      "  3250K .......... .......... .......... .......... .......... 68%  252M 0s\n",
      "  3300K .......... .......... .......... .......... .......... 69%  305M 0s\n",
      "  3350K .......... .......... .......... .......... .......... 70% 44.4M 0s\n",
      "  3400K .......... .......... .......... .......... .......... 71%  292M 0s\n",
      "  3450K .......... .......... .......... .......... .......... 72% 1.42M 0s\n",
      "  3500K .......... .......... .......... .......... .......... 73%  291M 0s\n",
      "  3550K .......... .......... .......... .......... .......... 74%  308M 0s\n",
      "  3600K .......... .......... .......... .......... .......... 75%  277M 0s\n",
      "  3650K .......... .......... .......... .......... .......... 76%  317M 0s\n",
      "  3700K .......... .......... .......... .......... .......... 77%  308M 0s\n",
      "  3750K .......... .......... .......... .......... .......... 79%  354M 0s\n",
      "  3800K .......... .......... .......... .......... .......... 80% 52.5M 0s\n",
      "  3850K .......... .......... .......... .......... .......... 81%  307M 0s\n",
      "  3900K .......... .......... .......... .......... .......... 82%  326M 0s\n",
      "  3950K .......... .......... .......... .......... .......... 83%  351M 0s\n",
      "  4000K .......... .......... .......... .......... .......... 84%  128M 0s\n",
      "  4050K .......... .......... .......... .......... .......... 85%  113M 0s\n",
      "  4100K .......... .......... .......... .......... .......... 86%  279M 0s\n",
      "  4150K .......... .......... .......... .......... .......... 87% 1.51M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 88% 90.0M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 89%  158M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 90%  227M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 91%  286M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 92%  301M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 93%  258M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 94%  259M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 95%  273M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 96%  272M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 97%  286M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 98%  252M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 99% 1.57M 0s\n",
      "  4800K ........                                              100%  143M=0.4s\n",
      "\n",
      "2022-05-21 13:31:11 (11.9 MB/s) - ‘ml-100k.zip’ saved [4924029/4924029]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "unzip -o ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/ml-100k\n",
      "870\t732\t2\t882123355\n",
      "86\t242\t4\t879569486\n",
      "303\t68\t4\t879467361\n",
      "551\t765\t1\t892785194\n",
      "404\t1238\t3\t883790181\n"
     ]
    }
   ],
   "source": [
    "%cd ml-100k\n",
    "!shuf ua.base -o ua.base.shuffled\n",
    "!head -5 ua.base.shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_users=943\n",
    "num_movies=1682\n",
    "num_features=num_users+num_movies\n",
    "\n",
    "num_ratings_train=90570\n",
    "num_ratings_test=9430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "def loadDataset(filename, lines, columns):\n",
    "    # Features are one-hot encoded in a sparse matrix\n",
    "    X = lil_matrix((lines, columns)).astype('float32')\n",
    "    # Labels are stored in a vector\n",
    "    Y = []\n",
    "    line=0\n",
    "    with open(filename,'r') as f:\n",
    "        samples=csv.reader(f,delimiter='\\t')\n",
    "        for userId,movieId,rating,timestamp in samples:\n",
    "            X[line,int(userId)-1] = 1\n",
    "            X[line,int(num_users)+int(movieId)-1] = 1\n",
    "            Y.append(int(rating))\n",
    "            line=line+1       \n",
    "    Y=np.array(Y).astype('float32')\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = loadDataset('ua.base.shuffled', num_ratings_train, num_features)\n",
    "X_test, Y_test = loadDataset('ua.test', num_ratings_test, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90570, 2625)\n",
      "(90570,)\n",
      "(9430, 2625)\n",
      "(9430,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "assert X_train.shape == (num_ratings_train, num_features)\n",
    "assert Y_train.shape == (num_ratings_train, )\n",
    "\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "assert X_test.shape  == (num_ratings_test, num_features)\n",
    "assert Y_test.shape  == (num_ratings_test, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to protobuf and save to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "prefix = 'fm-movielens'\n",
    "\n",
    "train_key      = 'train.protobuf'\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "\n",
    "test_key       = 'test.protobuf'\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test')\n",
    "\n",
    "output_prefix  = 's3://{}/{}/output'.format(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BytesIO object at 0x7fade2ae9eb8>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.BytesIO object at 0x7fade2ae9fc0>\n",
      "s3://sagemaker-us-east-1-608009264409/fm-movielens/train/train.protobuf\n",
      "s3://sagemaker-us-east-1-608009264409/fm-movielens/test/test.protobuf\n",
      "Output: s3://sagemaker-us-east-1-608009264409/fm-movielens/output\n"
     ]
    }
   ],
   "source": [
    "import io, boto3\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "def writeDatasetToProtobuf(X, Y, bucket, prefix, key):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, Y)\n",
    "    # use smac.write_numpy_to_dense_tensor(buf, feature, label) for numpy arrays\n",
    "    buf.seek(0)\n",
    "    print(buf)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)\n",
    "    \n",
    "train_data = writeDatasetToProtobuf(X_train, Y_train, bucket, train_prefix, train_key)    \n",
    "test_data  = writeDatasetToProtobuf(X_test, Y_test, bucket, test_prefix, test_key)    \n",
    "  \n",
    "print(train_data)\n",
    "print(test_data)\n",
    "print('Output: {}'.format(output_prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "region = sess.boto_session.region_name      \n",
    "container = retrieve('factorization-machines', region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-21 13:34:32 Starting - Starting the training job...\n",
      "2022-05-21 13:34:55 Starting - Preparing the instances for trainingProfilerReport-1653140071: InProgress\n",
      ".........\n",
      "2022-05-21 13:36:16 Downloading - Downloading input data...\n",
      "2022-05-21 13:36:56 Training - Downloading the training image.....\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:87: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:120: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:40 INFO 139950227810112] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:40 INFO 139950227810112] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'epochs': '10', 'feature_dim': '2625', 'num_factors': '64', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:40 INFO 139950227810112] Final configuration: {'epochs': '10', 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '2625', 'num_factors': '64', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:40 WARNING 139950227810112] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:40 INFO 139950227810112] Using default worker.\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:40 INFO 139950227810112] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2022-05-21 13:37:40.529] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2022-05-21 13:37:40.549] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 38, \"num_examples\": 1, \"num_bytes\": 64000}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:40 INFO 139950227810112] nvidia-smi: took 0.035 seconds to run.\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:40 INFO 139950227810112] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:40 INFO 139950227810112] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:40 INFO 139950227810112] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:40 INFO 139950227810112] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140260.5099006, \"EndTime\": 1653140260.5912943, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 51.02801322937012, \"count\": 1, \"min\": 51.02801322937012, \"max\": 51.02801322937012}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140260.5917616, \"EndTime\": 1653140260.5918117, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[13:37:40] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.206339.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[13:37:40] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.206339.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:40 INFO 139950227810112] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=3.692503358791634\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:40 INFO 139950227810112] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=13.6345810546875\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:40 INFO 139950227810112] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=3.51364208984375\u001b[0m\n",
      "\u001b[34m[2022-05-21 13:37:42.216] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 1570, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:42 INFO 139950227810112] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=1.747804583609713\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:42 INFO 139950227810112] #quality_metric: host=algo-1, epoch=0, train mse <loss>=3.054820862487122\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:42 INFO 139950227810112] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=1.3853263657119248\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140260.5916953, \"EndTime\": 1653140262.218401, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"update.time\": {\"sum\": 1625.962257385254, \"count\": 1, \"min\": 1625.962257385254, \"max\": 1625.962257385254}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:42 INFO 139950227810112] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140260.5924034, \"EndTime\": 1653140262.2191732, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 91570.0, \"count\": 1, \"min\": 91570, \"max\": 91570}, \"Total Batches Seen\": {\"sum\": 92.0, \"count\": 1, \"min\": 92, \"max\": 92}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:42 INFO 139950227810112] #throughput_metric: host=algo-1, train throughput=55659.00576651145 records/second\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:42 INFO 139950227810112] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=1.1261099125557195\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:42 INFO 139950227810112] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=1.26812353515625\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:42 INFO 139950227810112] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=0.950470703125\u001b[0m\n",
      "\u001b[34m[2022-05-21 13:37:44.191] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 1969, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:44 INFO 139950227810112] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=1.1315511752787142\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:44 INFO 139950227810112] #quality_metric: host=algo-1, epoch=1, train mse <loss>=1.2804080622746394\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:44 INFO 139950227810112] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=0.946644772707761\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140262.2187622, \"EndTime\": 1653140264.1930377, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1972.5432395935059, \"count\": 1, \"min\": 1972.5432395935059, \"max\": 1972.5432395935059}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:44 INFO 139950227810112] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140262.2204652, \"EndTime\": 1653140264.1936815, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 182140.0, \"count\": 1, \"min\": 182140, \"max\": 182140}, \"Total Batches Seen\": {\"sum\": 183.0, \"count\": 1, \"min\": 183, \"max\": 183}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:44 INFO 139950227810112] #throughput_metric: host=algo-1, train throughput=45890.89313244713 records/second\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:44 INFO 139950227810112] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=1.110156414936652\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:44 INFO 139950227810112] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=1.232447265625\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:44 INFO 139950227810112] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=0.938155517578125\u001b[0m\n",
      "\u001b[34m[2022-05-21 13:37:45.672] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 1473, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:45 INFO 139950227810112] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=1.1136732868386467\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:45 INFO 139950227810112] #quality_metric: host=algo-1, epoch=2, train mse <loss>=1.2402681898179946\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:45 INFO 139950227810112] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=0.9297577440869678\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140264.1933365, \"EndTime\": 1653140265.6725721, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1477.8337478637695, \"count\": 1, \"min\": 1477.8337478637695, \"max\": 1477.8337478637695}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:45 INFO 139950227810112] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140264.1947107, \"EndTime\": 1653140265.6727924, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 272710.0, \"count\": 1, \"min\": 272710, \"max\": 272710}, \"Total Batches Seen\": {\"sum\": 274.0, \"count\": 1, \"min\": 274, \"max\": 274}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:45 INFO 139950227810112] #throughput_metric: host=algo-1, train throughput=61270.307669181275 records/second\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:45 INFO 139950227810112] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=1.0918094951193695\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:45 INFO 139950227810112] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=1.1920479736328125\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:45 INFO 139950227810112] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=0.9220841674804687\u001b[0m\n",
      "\u001b[34m[2022-05-21 13:37:47.003] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 1328, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:47 INFO 139950227810112] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=1.0946224799664204\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:47 INFO 139950227810112] #quality_metric: host=algo-1, epoch=3, train mse <loss>=1.1981983736478365\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:47 INFO 139950227810112] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=0.9107956348461109\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140265.6726387, \"EndTime\": 1653140267.005003, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1331.6223621368408, \"count\": 1, \"min\": 1331.6223621368408, \"max\": 1331.6223621368408}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:47 INFO 139950227810112] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140265.673351, \"EndTime\": 1653140267.0055456, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 363280.0, \"count\": 1, \"min\": 363280, \"max\": 363280}, \"Total Batches Seen\": {\"sum\": 365.0, \"count\": 1, \"min\": 365, \"max\": 365}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:47 INFO 139950227810112] #throughput_metric: host=algo-1, train throughput=67977.29210750516 records/second\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:47 INFO 139950227810112] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=1.072811749016632\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:47 INFO 139950227810112] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=1.150925048828125\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:47 INFO 139950227810112] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=0.9044923706054687\u001b[0m\n",
      "\u001b[34m[2022-05-21 13:37:48.504] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 1495, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:48 INFO 139950227810112] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=1.075465320044686\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:48 INFO 139950227810112] #quality_metric: host=algo-1, epoch=4, train mse <loss>=1.1566256546188187\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:48 INFO 139950227810112] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=0.890601082267342\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140267.0050864, \"EndTime\": 1653140268.5059273, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1499.316692352295, \"count\": 1, \"min\": 1499.316692352295, \"max\": 1499.316692352295}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:48 INFO 139950227810112] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140267.0065794, \"EndTime\": 1653140268.5064876, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 453850.0, \"count\": 1, \"min\": 453850, \"max\": 453850}, \"Total Batches Seen\": {\"sum\": 456.0, \"count\": 1, \"min\": 456, \"max\": 456}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:48 INFO 139950227810112] #throughput_metric: host=algo-1, train throughput=60377.68717724328 records/second\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:48 INFO 139950227810112] #quality_metric: host=algo-1, epoch=5, batch=0 train rmse <loss>=1.054489275353684\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:48 INFO 139950227810112] #quality_metric: host=algo-1, epoch=5, batch=0 train mse <loss>=1.1119476318359376\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:48 INFO 139950227810112] #quality_metric: host=algo-1, epoch=5, batch=0 train absolute_loss <loss>=0.8864827270507812\u001b[0m\n",
      "\u001b[34m[2022-05-21 13:37:49.780] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 1271, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:49 INFO 139950227810112] #quality_metric: host=algo-1, epoch=5, train rmse <loss>=1.0572710696476564\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:49 INFO 139950227810112] #quality_metric: host=algo-1, epoch=5, train mse <loss>=1.1178221147138994\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:49 INFO 139950227810112] #quality_metric: host=algo-1, epoch=5, train absolute_loss <loss>=0.8702526996319111\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140268.5060103, \"EndTime\": 1653140269.7817645, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1274.1928100585938, \"count\": 1, \"min\": 1274.1928100585938, \"max\": 1274.1928100585938}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:49 INFO 139950227810112] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140268.5075252, \"EndTime\": 1653140269.7821925, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 544420.0, \"count\": 1, \"min\": 544420, \"max\": 544420}, \"Total Batches Seen\": {\"sum\": 547.0, \"count\": 1, \"min\": 547, \"max\": 547}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:49 INFO 139950227810112] #throughput_metric: host=algo-1, train throughput=71043.6183726128 records/second\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:49 INFO 139950227810112] #quality_metric: host=algo-1, epoch=6, batch=0 train rmse <loss>=1.037614416224935\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:49 INFO 139950227810112] #quality_metric: host=algo-1, epoch=6, batch=0 train mse <loss>=1.0766436767578125\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:49 INFO 139950227810112] #quality_metric: host=algo-1, epoch=6, batch=0 train absolute_loss <loss>=0.8687962036132812\u001b[0m\n",
      "\u001b[34m[2022-05-21 13:37:51.201] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 1416, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:51 INFO 139950227810112] #quality_metric: host=algo-1, epoch=6, train rmse <loss>=1.0406539774282568\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:51 INFO 139950227810112] #quality_metric: host=algo-1, epoch=6, train mse <loss>=1.082960700737251\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:51 INFO 139950227810112] #quality_metric: host=algo-1, epoch=6, train absolute_loss <loss>=0.8507913469587054\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140269.7819145, \"EndTime\": 1653140271.2024798, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1419.4304943084717, \"count\": 1, \"min\": 1419.4304943084717, \"max\": 1419.4304943084717}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:51 INFO 139950227810112] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140269.783012, \"EndTime\": 1653140271.2028668, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 634990.0, \"count\": 1, \"min\": 634990, \"max\": 634990}, \"Total Batches Seen\": {\"sum\": 638.0, \"count\": 1, \"min\": 638, \"max\": 638}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:51 INFO 139950227810112] #throughput_metric: host=algo-1, train throughput=63781.997275634865 records/second\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:51 INFO 139950227810112] #quality_metric: host=algo-1, epoch=7, batch=0 train rmse <loss>=1.0225249390487072\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:51 INFO 139950227810112] #quality_metric: host=algo-1, epoch=7, batch=0 train mse <loss>=1.0455572509765625\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:51 INFO 139950227810112] #quality_metric: host=algo-1, epoch=7, batch=0 train absolute_loss <loss>=0.8520836181640625\u001b[0m\n",
      "\n",
      "2022-05-21 13:37:56 Training - Training image download completed. Training in progress.\u001b[34m[2022-05-21 13:37:52.680] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 1475, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:52 INFO 139950227810112] #quality_metric: host=algo-1, epoch=7, train rmse <loss>=1.0258902911635157\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:52 INFO 139950227810112] #quality_metric: host=algo-1, epoch=7, train mse <loss>=1.052450889503563\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:52 INFO 139950227810112] #quality_metric: host=algo-1, epoch=7, train absolute_loss <loss>=0.8333817588051596\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140271.2025604, \"EndTime\": 1653140272.6815393, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1477.759838104248, \"count\": 1, \"min\": 1477.759838104248, \"max\": 1477.759838104248}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:52 INFO 139950227810112] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140271.2037525, \"EndTime\": 1653140272.681764, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 725560.0, \"count\": 1, \"min\": 725560, \"max\": 725560}, \"Total Batches Seen\": {\"sum\": 729.0, \"count\": 1, \"min\": 729, \"max\": 729}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:52 INFO 139950227810112] #throughput_metric: host=algo-1, train throughput=61272.81786359585 records/second\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:52 INFO 139950227810112] #quality_metric: host=algo-1, epoch=8, batch=0 train rmse <loss>=1.0092858356031846\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:52 INFO 139950227810112] #quality_metric: host=algo-1, epoch=8, batch=0 train mse <loss>=1.0186578979492187\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:52 INFO 139950227810112] #quality_metric: host=algo-1, epoch=8, batch=0 train absolute_loss <loss>=0.8369930419921875\u001b[0m\n",
      "\u001b[34m[2022-05-21 13:37:54.151] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 1466, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:54 INFO 139950227810112] #quality_metric: host=algo-1, epoch=8, train rmse <loss>=1.0130285369629846\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:54 INFO 139950227810112] #quality_metric: host=algo-1, epoch=8, train mse <loss>=1.026226816701365\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:54 INFO 139950227810112] #quality_metric: host=algo-1, epoch=8, train absolute_loss <loss>=0.8186013921381353\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140272.6816077, \"EndTime\": 1653140274.1516988, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1469.2585468292236, \"count\": 1, \"min\": 1469.2585468292236, \"max\": 1469.2585468292236}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:54 INFO 139950227810112] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140272.6824117, \"EndTime\": 1653140274.1519284, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 816130.0, \"count\": 1, \"min\": 816130, \"max\": 816130}, \"Total Batches Seen\": {\"sum\": 820.0, \"count\": 1, \"min\": 820, \"max\": 820}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:54 INFO 139950227810112] #throughput_metric: host=algo-1, train throughput=61627.07667229654 records/second\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:54 INFO 139950227810112] #quality_metric: host=algo-1, epoch=9, batch=0 train rmse <loss>=0.9978116314003422\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:54 INFO 139950227810112] #quality_metric: host=algo-1, epoch=9, batch=0 train mse <loss>=0.9956280517578125\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:54 INFO 139950227810112] #quality_metric: host=algo-1, epoch=9, batch=0 train absolute_loss <loss>=0.8241039428710938\u001b[0m\n",
      "\u001b[34m[2022-05-21 13:37:55.473] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 1316, \"num_examples\": 91, \"num_bytes\": 5796480}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] #quality_metric: host=algo-1, epoch=9, train rmse <loss>=1.0019729769510646\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] #quality_metric: host=algo-1, epoch=9, train mse <loss>=1.0039498465401786\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] #quality_metric: host=algo-1, epoch=9, train absolute_loss <loss>=0.8063859494387449\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] #quality_metric: host=algo-1, train rmse <loss>=1.0019729769510646\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] #quality_metric: host=algo-1, train mse <loss>=1.0039498465401786\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] #quality_metric: host=algo-1, train absolute_loss <loss>=0.8063859494387449\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140274.1517663, \"EndTime\": 1653140275.4740207, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 1321.3846683502197, \"count\": 1, \"min\": 1321.3846683502197, \"max\": 1321.3846683502197}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140274.1526055, \"EndTime\": 1653140275.4742482, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 906700.0, \"count\": 1, \"min\": 906700, \"max\": 906700}, \"Total Batches Seen\": {\"sum\": 911.0, \"count\": 1, \"min\": 911, \"max\": 911}, \"Max Records Seen Between Resets\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Max Batches Seen Between Resets\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}, \"Reset Count\": {\"sum\": 11.0, \"count\": 1, \"min\": 11, \"max\": 11}, \"Number of Records Since Last Reset\": {\"sum\": 90570.0, \"count\": 1, \"min\": 90570, \"max\": 90570}, \"Number of Batches Since Last Reset\": {\"sum\": 91.0, \"count\": 1, \"min\": 91, \"max\": 91}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] #throughput_metric: host=algo-1, train throughput=68516.98680318186 records/second\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 WARNING 139950227810112] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140275.474092, \"EndTime\": 1653140275.4778419, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 3.2901763916015625, \"count\": 1, \"min\": 3.2901763916015625, \"max\": 3.2901763916015625}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] Saved checkpoint to \"/tmp/tmpgsqxu72p/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2022-05-21 13:37:55.489] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 14959, \"num_examples\": 1, \"num_bytes\": 64000}\u001b[0m\n",
      "\u001b[34m[2022-05-21 13:37:55.569] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 79, \"num_examples\": 10, \"num_bytes\": 603520}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140275.4891746, \"EndTime\": 1653140275.571146, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 9430.0, \"count\": 1, \"min\": 9430, \"max\": 9430}, \"Total Batches Seen\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Max Records Seen Between Resets\": {\"sum\": 9430.0, \"count\": 1, \"min\": 9430, \"max\": 9430}, \"Max Batches Seen Between Resets\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 9430.0, \"count\": 1, \"min\": 9430, \"max\": 9430}, \"Number of Batches Since Last Reset\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}}}\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] #test_score (algo-1) : ('rmse', 1.022588374040208)\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] #test_score (algo-1) : ('mse', 1.0456869827221964)\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] #test_score (algo-1) : ('absolute_loss', 0.8421286590026014)\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] #quality_metric: host=algo-1, test rmse <loss>=1.022588374040208\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] #quality_metric: host=algo-1, test mse <loss>=1.0456869827221964\u001b[0m\n",
      "\u001b[34m[05/21/2022 13:37:55 INFO 139950227810112] #quality_metric: host=algo-1, test absolute_loss <loss>=0.8421286590026014\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1653140275.477916, \"EndTime\": 1653140275.572288, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 45.0594425201416, \"count\": 1, \"min\": 45.0594425201416, \"max\": 45.0594425201416}, \"totaltime\": {\"sum\": 15118.138790130615, \"count\": 1, \"min\": 15118.138790130615, \"max\": 15118.138790130615}}}\u001b[0m\n",
      "\n",
      "2022-05-21 13:38:16 Uploading - Uploading generated training model\n",
      "2022-05-21 13:38:16 Completed - Training job completed\n",
      "Training seconds: 127\n",
      "Billable seconds: 127\n"
     ]
    }
   ],
   "source": [
    "fm = Estimator(\n",
    "    container,\n",
    "    role=get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    output_path=output_prefix)\n",
    "\n",
    "fm.set_hyperparameters(\n",
    "    feature_dim=num_features,\n",
    "    predictor_type='regressor',\n",
    "    num_factors=64,\n",
    "    epochs=10)\n",
    "\n",
    "fm.fit({'train': train_data, 'test': test_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'fm-movielens-100k'\n",
    "\n",
    "fm_predictor = fm.deploy(\n",
    "    endpoint_name=endpoint_name,\n",
    "    instance_type='ml.t2.medium',\n",
    "    initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "\n",
    "class FMSerializer(JSONSerializer):\n",
    "    def serialize(self, data):\n",
    "       js = {'instances': []}\n",
    "       for row in data:\n",
    "              js['instances'].append({'features': row.tolist()})\n",
    "       return json.dumps(js)\n",
    "\n",
    "fm_predictor.serializer = FMSerializer()\n",
    "fm_predictor.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'score': 3.377105236053467}, {'score': 3.41229248046875}, {'score': 3.6281182765960693}]}\n"
     ]
    }
   ],
   "source": [
    "result = fm_predictor.predict(X_test[:3].toarray())\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
